{"sta":1,"data":{"subTitle":"大规格文件的上传优化","content":"<p>在开发过程中，收到这样一个问题反馈，在网站上传 100 MB 以上的文件经常失败，重试也要等老半天，这就难为需要上传大规格文件的用户了。那么应该怎么做才能快速上传，就算失败了再次发送也能从上次中断的地方继续上传呢？下文为你揭晓答案~<br></p><p>温馨提示：配合&nbsp;<a href=\"https://github.com/codefe/fileUploadDemo\" target=\"_blank\">Demo 源码</a>&nbsp;一起阅读效果更佳</p><p><h3>整体思路\n</h3>第一步是结合项目背景，调研比较优化的解决方案。 文件上传失败是老生常谈的问题，常用方案是将一个大文件切片成多个小文件，并行请求接口进行上传，所有请求得到响应后，在服务器端合并所有的分片文件。当分片上传失败，可以在重新上传时进行判断，只上传上次失败的部分，减少用户的等待时间，缓解服务器压力。这就是分片上传文件。\n<br>\n<br>大文件上传\n<br>那么如何实现大文件分片上传呢？\n<br>\n<br>流程图如下：</p><p><img src=\"./app/json/js/bigdata/img/1.png\" style=\"max-width:100%;\" class=\"\"></p><p>分为以下步骤实现：\n<br>\n<br><h3>1. 文件 MD5 加密\n</h3>MD5 是文件的唯一标识，可以利用文件的 MD5 查询文件的上传状态。\n<br>\n<br>根据文件的修改时间、文件名称、最后修改时间等信息，通过 spark-md5 生成文件的 MD5。需要注意的是，大规格文件需要分片读取文件，将读取的文件内容添加到 spark-md5 的 hash 计算中，直到文件读取完毕，最后返回最终的 hash 码到 callback 回调函数里面。这里可以根据需要添加文件读取的进度条。</p><p><img src=\"./app/json/js/bigdata/img/2.gif\" style=\"max-width:100%;\" class=\"\"></p><p>实现方法如下：</p><pre><code>// 修改时间+文件名称+最后修改时间--&gt;MD5\nmd5File (file) {\n  return new Promise((resolve, reject) =&gt; {\n    let blobSlice =\n      File.prototype.slice ||\n      File.prototype.mozSlice ||\n      File.prototype.webkitSlice\n    let chunkSize = file.size / 100\n    let chunks = 100\n    let currentChunk = 0\n    let spark = new SparkMD5.ArrayBuffer()\n    let fileReader = new FileReader()\n    fileReader.onload = function (e) {\n      console.log('read chunk nr', currentChunk + 1, 'of', chunks)\n      spark.append(e.target.result) // Append array buffer\n      currentChunk++\n      if (currentChunk &lt; chunks) {\n        loadNext()\n      } else {\n        let cur = +new Date()\n        console.log('finished loading')\n        // alert(spark.end() + '---' + (cur - pre)); // Compute hash\n        let result = spark.end()\n        resolve(result)\n      }\n    }\n    fileReader.onerror = function (err) {\n      console.warn('oops, something went wrong.')\n      reject(err)\n    }\n    function loadNext () {\n      let start = currentChunk * chunkSize\n      let end =\n        start + chunkSize &gt;= file.size ? file.size : start + chunkSize\n      fileReader.readAsArrayBuffer(blobSlice.call(file, start, end))\n    }\n    loadNext()\n  })\n}</code></pre><p><h3>2. 查询文件状态\n</h3>前端得到文件的 MD5 后，从后台查询是否存在名称为 MD5 的文件夹，如果存在，列出文件夹下所有文件，得到已上传的切片列表，如果不存在，则已上传的切片列表为空。</p><p><img src=\"./app/json/js/bigdata/img/3.png\" style=\"max-width:100%;\" class=\"\"></p><pre><code>// 校验文件的MD5\ncheckFileMD5 (file, fileName, fileMd5Value, onError) {\n  const fileSize = file.size\n  const { chunkSize, uploadProgress } = this\n  this.chunks = Math.ceil(fileSize / chunkSize)\n  return new Promise(async (resolve, reject) =&gt; {\n    const params = {\n      fileName: fileName,\n      fileMd5Value: fileMd5Value,\n    }\n    const { ok, data } = await services.checkFile(params)\n    if (ok) {\n      this.hasUploaded = data.chunkList.length\n      uploadProgress(file)\n      resolve(data)\n    } else {\n      reject(ok)\n      onError()\n    }\n  })\n}</code></pre><p><h3>3. 文件分片\n</h3>文件上传优化的核心就是文件分片，Blob 对象中的 slice 方法可以对文件进行切割，File 对象是继承 Blob 对象的，因此 File 对象也有 slice 方法。\n<br>\n<br>定义每一个分片文件的大小变量为 chunkSize，通过文件大小 FileSize 和分片大小 chunkSize 得到分片数量 chunks，使用 for 循环和 file.slice() 方法对文件进行分片，序号为 0 - n，和已上传的切片列表做比对，得到所有未上传的分片，push 到请求列表 requestList。</p><pre><code>async checkAndUploadChunk (file, fileMd5Value, chunkList) {\n  let { chunks, upload } = this\n  const requestList = []\n  for (let i = 0; i &lt; chunks; i++) {\n    let exit = chunkList.indexOf(i + '') &gt; -1\n    // 如果已经存在, 则不用再上传当前块\n    if (!exit) {\n      requestList.push(upload(i, fileMd5Value, file))\n    }\n  }\n  console.log({ requestList })\n  const result =\n    requestList.length &gt; 0\n      ? await Promise.all(requestList)\n        .then(result =&gt; {\n          console.log({ result })\n          return result.every(i =&gt; i.ok)\n        })\n        .catch(err =&gt; {\n          return err\n        })\n      : true\n  console.log({ result })\n  return result === true\n}</code></pre><p><h3>4. 上传分片\n</h3>调用 Promise.all 并发上传所有的切片，将切片序号、切片文件、文件 MD5 传给后台。\n<br>\n<br>后台接收到上传请求后，首先查看名称为文件 MD5 的文件夹是否存在，不存在则创建文件夹，然后通过 fs-extra 的 rename 方法，将切片从临时路径移动切片文件夹中，</p><p>当全部分片上传成功，通知服务端进行合并，当有一个分片上传失败时，提示“上传失败”。在重新上传时，通过文件 MD5 得到文件的上传状态，当服务器已经有该 MD5 对应的切片时，代表该切片已经上传过，无需再次上传，当服务器找不到该 MD5 对应的切片时，代表该切片需要上传，用户只需上传这部分切片，就可以完整上传整个文件，这就是文件的断点续传。</p><p><img src=\"./app/json/js/bigdata/img/4.gif\" style=\"max-width:100%;\"></p><pre><code>// 上传chunk\nupload (i, fileMd5Value, file) {\n  const { uploadProgress, chunks } = this\n  return new Promise((resolve, reject) =&gt; {\n    let { chunkSize } = this\n    // 构造一个表单，FormData是HTML5新增的\n    let end =\n      (i + 1) * chunkSize &gt;= file.size ? file.size : (i + 1) * chunkSize\n    let form = new FormData()\n    form.append('data', file.slice(i * chunkSize, end)) // file对象的slice方法用于切出文件的一部分\n    form.append('total', chunks) // 总片数\n    form.append('index', i) // 当前是第几片\n    form.append('fileMd5Value', fileMd5Value)\n    services\n      .uploadLarge(form)\n      .then(data =&gt; {\n        if (data.ok) {\n          this.hasUploaded++\n          uploadProgress(file)\n        }\n        console.log({ data })\n        resolve(data)\n      })\n      .catch(err =&gt; {\n        reject(err)\n      })\n  })\n}</code></pre><p><h3>5. 上传进度\n</h3>虽然分片批量上传比大文件单次上传会快很多，也还是有一段加载时间，这时应该加上上传进度的提示，实时显示文件上传进度。\n<br>\n<br>原生 Javascript 的 XMLHttpRequest 有提供 progress 事件，这个事件会返回文件已上传的大小和总大小。项目使用 axios 对 ajax 进行封装，可以在 config 中增加 onUploadProgress 方法，监听文件上传进度。</p><pre><code>const config = {\n  onUploadProgress: progressEvent =&gt; {\n    var complete = (progressEvent.loaded / progressEvent.total * 100 | 0) + '%'\n  }\n}\nservices.uploadChunk(form, config)</code></pre><p><h3>6. 合并分片\n</h3>上传完所有文件分片后，前端主动通知服务端进行合并，服务端接受到这个请求时主动合并切片，通过文件 MD5 在服务器的文件上传路径中找到同名文件夹。从上文可知，文件分片是按照分片序号命名的，而分片上传接口是异步的，无法保证服务器接收到的切片是按照请求顺序拼接。所以应该在合并文件夹里的分片文件前，根据文件名进行排序，然后再通过 concat-files 合并分片文件，得到用户上传的文件。至此大文件上传就完成了。</p><p>Node 端代码：</p><pre><code>// 合并文件\nexports.merge = {\n  validate: {\n    query: {\n      fileName: Joi.string()\n        .trim()\n        .required()\n        .description('文件名称'),\n      md5: Joi.string()\n        .trim()\n        .required()\n        .description('文件md5'),\n      size: Joi.string()\n        .trim()\n        .required()\n        .description('文件大小'),\n    },\n  },\n  permission: {\n    roles: ['user'],\n  },\n  async handler (ctx) {\n    const { fileName, md5, size } = ctx.request.query\n    let { name, base: filename, ext } = path.parse(fileName)\n    const newFileName = randomFilename(name, ext)\n    await mergeFiles(path.join(uploadDir, md5), uploadDir, newFileName, size)\n      .then(async () =&gt; {\n        const file = {\n          key: newFileName,\n          name: filename,\n          mime_type: mime.getType(`${uploadDir}/${newFileName}`),\n          ext,\n          path: `${uploadDir}/${newFileName}`,\n          provider: 'oss',\n          size,\n          owner: ctx.state.user.id,\n        }\n        const key = encodeURIComponent(file.key)\n          .replace(/%/g, '')\n          .slice(-100)\n        file.url = await uploadLocalFileToOss(file.path, key)\n        file.url = getFileUrl(file)\n        const f = await File.create(omit(file, 'path'))\n        const files = []\n        files.push(f)\n        ctx.body = invokeMap(files, 'toJSON')\n      })\n      .catch(() =&gt; {\n        throw Boom.badData('大文件分片合并失败，请稍候重试~')\n      })\n  },\n}</code></pre><p><h3>总结\n</h3>本文讲述了大规格文件上传优化的一些做法，总结为以下 4 点：\n<br>\n<br><blockquote>1. Blob.slice 将文件切片，并发上传多个切片，所有切片上传后告知服务器合并，实现大文件分片上传；<br>\n2. 原生 XMLHttpRequest 的 onprogress 对切片上传进度的监听，实时获取文件上传进度；<br>\n3. spark-md5 根据文件内容算出文件 MD5，得到文件唯一标识，与文件上传状态绑定；<br>\n4. 分片上传前通过文件 MD5 查询已上传切片列表，上传时只上传未上传过的切片，实现断点续传。</blockquote></p><p><br></p><p><br></p>"}}